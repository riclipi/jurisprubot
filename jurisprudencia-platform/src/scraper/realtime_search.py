"""
Sistema de busca em tempo real no TJSP com estratÃ©gias anti-detecÃ§Ã£o
Busca diretamente no site oficial e aplica busca semÃ¢ntica
"""

import requests
from bs4 import BeautifulSoup
import time
import logging
from urllib.parse import urljoin, urlparse
import re
import random
import json
from typing import List, Dict, Optional
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from datetime import datetime

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AdvancedRealtimeSearch:
    """
    Sistema avanÃ§ado de busca em tempo real no TJSP com anti-detecÃ§Ã£o
    Combina web scraping stealth com busca semÃ¢ntica
    """
    
    def __init__(self, max_results=20):
        """
        Inicializa o sistema de busca avanÃ§ado
        
        Args:
            max_results: NÃºmero mÃ¡ximo de resultados por busca
        """
        logger.info("ğŸš€ Inicializando sistema avanÃ§ado de busca em tempo real...")
        
        self.max_results = max_results
        self.base_url = "https://esaj.tjsp.jus.br"
        self.search_url = f"{self.base_url}/cjsg/consultaCompleta.do"
        
        # Pool de User-Agents realistas
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:120.0) Gecko/20100101 Firefox/120.0',
            'Mozilla/5.0 (X11; Linux x86_64; rv:120.0) Gecko/20100101 Firefox/120.0'
        ]
        
        # Configurar sessÃ£o
        self.session = requests.Session()
        self.setup_realistic_headers()
        
        # ConfiguraÃ§Ãµes de timing
        self.min_delay = 3
        self.max_delay = 7
        self.request_timeout = 30
        
        # EstatÃ­sticas de tentativas
        self.stats = {
            'total_attempts': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'anti_bot_blocks': 0,
            'fallback_usage': 0
        }
        
        # Carregar modelo de embeddings
        logger.info("ğŸ“¥ Carregando modelo de embeddings...")
        try:
            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            logger.info("âœ… Modelo carregado com sucesso!")
        except Exception as e:
            logger.error(f"âŒ Erro ao carregar modelo: {e}")
            self.embedding_model = None
        
        # Cache para resultados
        self.cache = {}
        
        logger.info("âœ… Sistema avanÃ§ado de busca inicializado!")

    def setup_realistic_headers(self):
        """Configura headers realistas para simular navegador real"""
        user_agent = random.choice(self.user_agents)
        
        # Headers que simulam comportamento humano real
        headers = {
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        }
        
        # Adicionar headers baseados no User-Agent
        if 'Chrome' in user_agent:
            headers.update({
                'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                'sec-ch-ua-mobile': '?0',
                'sec-ch-ua-platform': '"Windows"' if 'Windows' in user_agent else '"macOS"' if 'Mac' in user_agent else '"Linux"'
            })
        
        self.session.headers.update(headers)
        logger.info(f"ğŸ”§ Headers configurados com User-Agent: {user_agent[:50]}...")

    def simulate_human_delay(self, base_delay=None):
        """Simula delay humano com variaÃ§Ã£o natural"""
        if base_delay is None:
            delay = random.uniform(self.min_delay, self.max_delay)
        else:
            # Adicionar variaÃ§Ã£o de Â±20% ao delay base
            variation = base_delay * 0.2
            delay = random.uniform(base_delay - variation, base_delay + variation)
        
        logger.info(f"â³ Aguardando {delay:.1f}s para simular comportamento humano...")
        time.sleep(delay)

    def warm_up_session(self):
        """Aquece a sessÃ£o acessando a pÃ¡gina inicial primeiro"""
        try:
            logger.info("ğŸŒ¡ï¸ Aquecendo sessÃ£o - acessando pÃ¡gina inicial...")
            
            # Acessar pÃ¡gina inicial para obter cookies
            warm_up_url = f"{self.base_url}/cjsg/consultaCompleta.do"
            
            response = self.session.get(warm_up_url, timeout=self.request_timeout)
            
            if response.status_code == 200:
                logger.info("âœ… SessÃ£o aquecida com sucesso!")
                # Extrair cookies importantes
                cookies = self.session.cookies.get_dict()
                if cookies:
                    logger.info(f"ğŸª Cookies obtidos: {len(cookies)} itens")
                return True
            else:
                logger.warning(f"âš ï¸ Aquecimento retornou status {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erro no aquecimento da sessÃ£o: {e}")
            return False

    def search_with_stealth(self, query: str, max_retries=3) -> List[Dict]:
        """
        Realiza busca com tÃ©cnicas stealth anti-detecÃ§Ã£o
        
        Args:
            query: Termo de busca
            max_retries: MÃ¡ximo de tentativas
            
        Returns:
            Lista de acÃ³rdÃ£os encontrados
        """
        self.stats['total_attempts'] += 1
        
        logger.info(f"ğŸ•µï¸ Iniciando busca stealth para: '{query}'")
        
        # Verificar cache primeiro
        cache_key = f"stealth_{query}"
        if cache_key in self.cache:
            logger.info("ğŸ“‹ Resultado encontrado no cache")
            return self.cache[cache_key]
        
        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ¯ Tentativa {attempt + 1}/{max_retries}")
                
                # Reconfigurar headers para cada tentativa
                self.setup_realistic_headers()
                
                # Aquecer sessÃ£o se primeira tentativa
                if attempt == 0:
                    if not self.warm_up_session():
                        logger.warning("âš ï¸ Falha no aquecimento, continuando mesmo assim...")
                    
                    # Delay adicional apÃ³s aquecimento
                    self.simulate_human_delay(2)
                
                # ParÃ¢metros de busca mais simples e realistas
                search_params = self.build_search_params(query)
                
                # Fazer request
                logger.info("ğŸŒ Enviando request para TJSP...")
                response = self.session.get(
                    self.search_url,
                    params=search_params,
                    timeout=self.request_timeout
                )
                
                # Analisar resposta
                status_code = response.status_code
                logger.info(f"ğŸ“¡ Status HTTP: {status_code}")
                
                if status_code == 200:
                    # Sucesso! Extrair resultados
                    self.stats['successful_requests'] += 1
                    
                    acordaos = self.extract_from_search_results(response.text)
                    
                    if acordaos:
                        self.cache[cache_key] = acordaos
                        logger.info(f"âœ… Busca bem-sucedida! {len(acordaos)} acÃ³rdÃ£os encontrados")
                        return acordaos
                    else:
                        logger.warning("âš ï¸ Nenhum resultado encontrado no HTML")
                        
                elif status_code == 400:
                    self.stats['anti_bot_blocks'] += 1
                    logger.warning(f"ğŸš« PossÃ­vel detecÃ§Ã£o anti-bot (400). Tentativa {attempt + 1}")
                    
                    if attempt < max_retries - 1:
                        # Delay maior antes de tentar novamente
                        logger.info("ğŸ”„ Aguardando antes de nova tentativa...")
                        self.simulate_human_delay(random.uniform(10, 20))
                        
                elif status_code == 403:
                    self.stats['anti_bot_blocks'] += 1
                    logger.error("ğŸš« Acesso negado (403) - possÃ­vel IP bloqueado")
                    break
                    
                elif status_code == 429:
                    self.stats['anti_bot_blocks'] += 1
                    logger.warning("ğŸš« Rate limit detectado (429)")
                    
                    if attempt < max_retries - 1:
                        wait_time = random.uniform(30, 60)
                        logger.info(f"â° Aguardando {wait_time:.1f}s devido ao rate limit...")
                        time.sleep(wait_time)
                        
                else:
                    logger.error(f"âŒ Status HTTP inesperado: {status_code}")
                
                # Log da resposta para debug
                self.log_response_details(response, attempt)
                
            except requests.exceptions.RequestException as e:
                logger.error(f"âŒ Erro de request na tentativa {attempt + 1}: {e}")
                self.stats['failed_requests'] += 1
                
                if attempt < max_retries - 1:
                    self.simulate_human_delay(5)
            
            except Exception as e:
                logger.error(f"âŒ Erro inesperado na tentativa {attempt + 1}: {e}")
                self.stats['failed_requests'] += 1
        
        # Se chegou aqui, todas as tentativas falharam
        logger.error("âŒ Todas as tentativas de busca stealth falharam")
        return []

    def build_search_params(self, query: str) -> Dict:
        """ConstrÃ³i parÃ¢metros de busca mais realistas"""
        return {
            'conversationId': '',
            'dadosConsulta.valorConsultaLivre': query,
            'dadosConsulta.valorConsulta': query,
            'dadosConsulta.tipoNuProcesso': 'UNIFICADO',
            'dadosConsulta.segredoJustica': 'false',
            'dadosConsulta.pesquisarComSinonimos': 'false',
            'dadosConsulta.tipoPesquisa': 'LIVRE',
            'dadosConsulta.ordenacao': 'RELEVANCIA',
            'contadoragente': '0',
            'contadorMagistrdo': '0',
            'dadosConsulta.tipoPagina': 'CONSULTA',
            'numeroRegistros': str(min(self.max_results, 20)),
        }

    def extract_from_search_results(self, html: str) -> List[Dict]:
        """Extrai resultados da pÃ¡gina de busca de forma mais robusta"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            acordaos = []
            
            # Procurar por diferentes estruturas de resultados
            result_selectors = [
                '.unj-resultado-pesquisa',
                '.resultado-pesquisa',
                '.resultado',
                '[class*="resultado"]',
                'tr[id*="resultado"]',
                '.fundocinza1'
            ]
            
            results_found = False
            
            for selector in result_selectors:
                items = soup.select(selector)
                
                if items:
                    logger.info(f"ğŸ¯ Encontrados {len(items)} itens com seletor: {selector}")
                    results_found = True
                    
                    for i, item in enumerate(items[:self.max_results]):
                        try:
                            acordao = self.extract_acordao_data(item, i)
                            if acordao:
                                acordaos.append(acordao)
                        except Exception as e:
                            logger.warning(f"âš ï¸ Erro ao extrair item {i}: {e}")
                    
                    break  # Usar o primeiro seletor que funcionar
            
            if not results_found:
                logger.warning("âš ï¸ Nenhum seletor de resultado funcionou")
                # Log do HTML para debug (apenas primeiros 1000 chars)
                logger.debug(f"HTML recebido (preview): {html[:1000]}...")
            
            return acordaos
            
        except Exception as e:
            logger.error(f"âŒ Erro ao extrair resultados: {e}")
            return []

    def extract_acordao_data(self, item_soup, index: int) -> Dict:
        """Extrai dados de um item de acÃ³rdÃ£o"""
        acordao = {
            'numero_processo': '',
            'relator': '',
            'data_julgamento': '',
            'ementa': '',
            'url_detalhes': '',
            'index': index
        }
        
        try:
            # Extrair nÃºmero do processo
            processo_patterns = [
                'a[href*="processo"]',
                'a[href*="numprocesso"]',
                '.numero-processo',
                '[class*="processo"]'
            ]
            
            for pattern in processo_patterns:
                link = item_soup.select_one(pattern)
                if link:
                    acordao['numero_processo'] = link.get_text(strip=True)
                    href = link.get('href', '')
                    if href:
                        acordao['url_detalhes'] = urljoin(self.base_url, href)
                    break
            
            # Extrair relator
            relator_text = item_soup.find(text=re.compile(r'(Relator|Des\.|Desembargador)', re.I))
            if relator_text:
                # Pegar o texto prÃ³ximo
                parent = relator_text.parent
                if parent:
                    acordao['relator'] = parent.get_text(strip=True)
            
            # Extrair data
            date_pattern = re.compile(r'\d{1,2}[/.-]\d{1,2}[/.-]\d{4}')
            date_match = item_soup.find(text=date_pattern)
            if date_match:
                acordao['data_julgamento'] = date_match.strip()
            
            # Extrair ementa/resumo
            ementa_selectors = [
                '.ementa',
                '.resumo',
                '.texto-resumo',
                '[class*="ementa"]',
                '[class*="resumo"]'
            ]
            
            for selector in ementa_selectors:
                ementa_elem = item_soup.select_one(selector)
                if ementa_elem:
                    acordao['ementa'] = ementa_elem.get_text(strip=True)
                    break
            
            # Se nÃ£o encontrou ementa especÃ­fica, pegar texto geral
            if not acordao['ementa']:
                full_text = item_soup.get_text(strip=True)
                # Pegar os primeiros 300 caracteres como ementa
                acordao['ementa'] = full_text[:300] + "..." if len(full_text) > 300 else full_text
            
            return acordao if acordao['numero_processo'] or acordao['ementa'] else None
            
        except Exception as e:
            logger.warning(f"âš ï¸ Erro ao extrair dados do acÃ³rdÃ£o {index}: {e}")
            return None

    def log_response_details(self, response, attempt: int):
        """Registra detalhes da resposta para debug"""
        details = {
            'attempt': attempt + 1,
            'status_code': response.status_code,
            'headers': dict(response.headers),
            'content_length': len(response.text),
            'url': response.url,
            'final_url': response.url != self.search_url
        }
        
        logger.info(f"ğŸ“‹ Detalhes da resposta: {json.dumps(details, indent=2, default=str)}")
        
        # Verificar por indicadores de bloqueio
        content_lower = response.text.lower()
        block_indicators = [
            'access denied',
            'blocked',
            'captcha',
            'robot',
            'bot detected',
            'security check',
            'unusual traffic'
        ]
        
        found_indicators = [indicator for indicator in block_indicators if indicator in content_lower]
        if found_indicators:
            logger.warning(f"ğŸš« Indicadores de bloqueio encontrados: {found_indicators}")

    def get_stats(self) -> Dict:
        """Retorna estatÃ­sticas de uso"""
        return {
            **self.stats,
            'success_rate': (self.stats['successful_requests'] / max(self.stats['total_attempts'], 1)) * 100,
            'cache_size': len(self.cache)
        }

    def search_tjsp_live(self, query: str) -> List[Dict]:
        """
        Realiza busca diretamente no site do TJSP (mÃ©todo compatÃ­vel)
        
        Args:
            query: Termo de busca
            
        Returns:
            Lista de dicionÃ¡rios com informaÃ§Ãµes dos acÃ³rdÃ£os
        """
        return self.search_with_stealth(query)

    def get_relevant_chunks(self, query: str) -> List[Dict]:
        """
        MÃ©todo principal: busca TJSP + extraÃ§Ã£o + busca semÃ¢ntica (mÃ©todo compatÃ­vel)
        
        Args:
            query: Consulta de busca
            
        Returns:
            Top 5 trechos mais relevantes
        """
        logger.info(f"ğŸ¯ Iniciando busca completa avanÃ§ada para: '{query}'")
        
        try:
            # 1. Buscar no TJSP com stealth
            acordaos = self.search_with_stealth(query)
            
            if not acordaos:
                logger.warning("âš ï¸ Nenhum acÃ³rdÃ£o encontrado no TJSP")
                return self._fallback_to_local_data(query)
            
            # 2. Extrair textos completos (limitado para nÃ£o sobrecarregar)
            max_extractions = min(3, len(acordaos))  # Reduzido para 3
            logger.info(f"ğŸ“„ Extraindo texto de {max_extractions} acÃ³rdÃ£os...")
            
            for i in range(max_extractions):
                acordao = acordaos[i]
                if acordao.get('url_detalhes'):
                    # Simular delay humano antes de cada extraÃ§Ã£o
                    if i > 0:
                        self.simulate_human_delay()
                    
                    texto_completo = self.get_acordao_full_text(acordao['url_detalhes'])
                    acordao['texto_completo'] = texto_completo
            
            # 3. Aplicar busca semÃ¢ntica
            ranked_acordaos = self.semantic_search(query, acordaos)
            
            # 4. Dividir em chunks e retornar top 5
            relevant_chunks = self._create_chunks_from_acordaos(ranked_acordaos, query)
            
            logger.info(f"âœ… Busca avanÃ§ada concluÃ­da. Retornando {len(relevant_chunks)} chunks")
            return relevant_chunks[:5]
            
        except Exception as e:
            logger.error(f"âŒ Erro na busca completa avanÃ§ada: {e}")
            return self._fallback_to_local_data(query)

    def get_acordao_full_text(self, acordao_url: str) -> str:
        """
        Extrai texto completo de um acÃ³rdÃ£o com tÃ©cnicas stealth
        
        Args:
            acordao_url: URL do acÃ³rdÃ£o
            
        Returns:
            Texto extraÃ­do do acÃ³rdÃ£o
        """
        if not acordao_url:
            return ""
        
        try:
            logger.info(f"ğŸ“„ Extraindo texto stealth de: {acordao_url}")
            
            # Configurar headers para simular navegaÃ§Ã£o humana
            self.setup_realistic_headers()
            
            # Fazer request com delay humano
            response = self.session.get(acordao_url, timeout=self.request_timeout)
            
            if response.status_code != 200:
                logger.warning(f"âš ï¸ Status {response.status_code} para {acordao_url}")
                return ""
            
            # Parse HTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Remover scripts e styles
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            
            # Extrair texto das seÃ§Ãµes relevantes
            text_sections = []
            
            # Procurar por seÃ§Ãµes especÃ­ficas de acÃ³rdÃ£o (mÃºltiplos seletores)
            selectors = [
                '.acordao-texto',
                '.texto-acordao', 
                '.conteudo-acordao',
                '#divImprimivel',
                '.documento-texto',
                '.integra-acordao',
                '.texto-completo',
                '[class*="texto"]',
                '[class*="acordao"]',
                '.conteudo'
            ]
            
            for selector in selectors:
                sections = soup.select(selector)
                for section in sections:
                    text = section.get_text(separator=' ', strip=True)
                    if len(text) > 100:  # SÃ³ incluir seÃ§Ãµes com conteÃºdo substancial
                        text_sections.append(text)
            
            # Se nÃ£o encontrou seÃ§Ãµes especÃ­ficas, pegar corpo geral
            if not text_sections:
                body = soup.find('body')
                if body:
                    text_sections.append(body.get_text(separator=' ', strip=True))
            
            # Combinar e limpar texto
            full_text = ' '.join(text_sections)
            full_text = re.sub(r'\s+', ' ', full_text)  # Normalizar espaÃ§os
            full_text = full_text.strip()
            
            logger.info(f"âœ… ExtraÃ­do: {len(full_text)} caracteres")
            return full_text
            
        except Exception as e:
            logger.error(f"âŒ Erro ao extrair texto de {acordao_url}: {e}")
            return ""

    def semantic_search(self, query: str, extracted_texts: List[Dict]) -> List[Dict]:
        """
        Aplica busca semÃ¢ntica nos textos extraÃ­dos (mÃ©todo compatÃ­vel)
        
        Args:
            query: Consulta de busca
            extracted_texts: Lista de textos extraÃ­dos
            
        Returns:
            Lista ordenada por relevÃ¢ncia semÃ¢ntica
        """
        if not self.embedding_model or not extracted_texts:
            logger.warning("âš ï¸ Modelo nÃ£o disponÃ­vel ou sem textos")
            return extracted_texts
        
        logger.info(f"ğŸ§  Aplicando busca semÃ¢ntica avanÃ§ada para: '{query}'")
        
        try:
            # Preparar textos para embedding
            texts_for_embedding = []
            valid_texts = []
            
            for item in extracted_texts:
                # Combinar ementa + texto completo para busca
                combined_text = f"{item.get('ementa', '')} {item.get('texto_completo', '')}".strip()
                
                if combined_text:
                    texts_for_embedding.append(combined_text)
                    valid_texts.append(item)
            
            if not texts_for_embedding:
                logger.warning("âš ï¸ Nenhum texto vÃ¡lido para embedding")
                return extracted_texts
            
            # Criar embeddings
            logger.info("ğŸ”¢ Criando embeddings avanÃ§ados...")
            query_embedding = self.embedding_model.encode([query])
            text_embeddings = self.embedding_model.encode(texts_for_embedding)
            
            # Calcular similaridades
            similarities = cosine_similarity(query_embedding, text_embeddings)[0]
            
            # Adicionar scores aos textos
            for i, similarity in enumerate(similarities):
                valid_texts[i]['similarity_score'] = float(similarity)
            
            # Ordenar por relevÃ¢ncia
            sorted_texts = sorted(valid_texts, key=lambda x: x['similarity_score'], reverse=True)
            
            logger.info(f"âœ… Busca semÃ¢ntica avanÃ§ada concluÃ­da. Top score: {sorted_texts[0]['similarity_score']:.3f}")
            return sorted_texts
            
        except Exception as e:
            logger.error(f"âŒ Erro na busca semÃ¢ntica avanÃ§ada: {e}")
            return extracted_texts

    def _create_chunks_from_acordaos(self, acordaos: List[Dict], query: str) -> List[Dict]:
        """Cria chunks relevantes a partir dos acÃ³rdÃ£os (mÃ©todo compatÃ­vel)"""
        chunks = []
        
        for i, acordao in enumerate(acordaos[:5]):  # Top 5 acÃ³rdÃ£os
            # Chunk da ementa
            if acordao.get('ementa'):
                chunks.append({
                    'rank': len(chunks) + 1,
                    'score': acordao.get('similarity_score', 0),
                    'text': acordao['ementa'],
                    'source': 'ementa',
                    'processo': acordao.get('numero_processo', 'N/A'),
                    'relator': acordao.get('relator', 'N/A'),
                    'data': acordao.get('data_julgamento', 'N/A'),
                    'url': acordao.get('url_detalhes', '')
                })
            
            # Chunks do texto completo
            if acordao.get('texto_completo'):
                texto = acordao['texto_completo']
                
                # Dividir em chunks de ~500 caracteres
                chunk_size = 500
                for j in range(0, len(texto), chunk_size):
                    chunk_text = texto[j:j+chunk_size]
                    
                    # Verificar se chunk contÃ©m palavras da query
                    query_words = query.lower().split()
                    chunk_lower = chunk_text.lower()
                    relevance = sum(1 for word in query_words if word in chunk_lower)
                    
                    if relevance > 0:  # SÃ³ incluir chunks relevantes
                        chunks.append({
                            'rank': len(chunks) + 1,
                            'score': acordao.get('similarity_score', 0) * relevance / len(query_words),
                            'text': chunk_text,
                            'source': 'texto_completo',
                            'processo': acordao.get('numero_processo', 'N/A'),
                            'relator': acordao.get('relator', 'N/A'),
                            'data': acordao.get('data_julgamento', 'N/A'),
                            'url': acordao.get('url_detalhes', '')
                        })
        
        # Ordenar por score
        chunks.sort(key=lambda x: x['score'], reverse=True)
        
        # Reordenar ranks
        for i, chunk in enumerate(chunks):
            chunk['rank'] = i + 1
        
        return chunks

    def _fallback_to_local_data(self, query: str) -> List[Dict]:
        """Fallback para dados locais se TJSP indisponÃ­vel (mÃ©todo compatÃ­vel)"""
        logger.info("ğŸ”„ Usando fallback para dados locais...")
        self.stats['fallback_usage'] += 1
        
        try:
            # Importar sistema local
            import sys
            sys.path.append('.')
            from ..rag.simple_search import SimpleSearchEngine
            
            # Usar sistema local
            local_search = SimpleSearchEngine()
            results = local_search.search(query, top_k=5)
            
            # Converter formato
            fallback_chunks = []
            for result in results:
                fallback_chunks.append({
                    'rank': result['rank'],
                    'score': result['score'],
                    'text': result['text'],
                    'source': 'local_data',
                    'processo': 'N/A',
                    'relator': 'N/A',
                    'data': 'N/A',
                    'url': '',
                    'arquivo_local': result['metadata']['file']
                })
            
            logger.info(f"âœ… Fallback: {len(fallback_chunks)} resultados locais")
            return fallback_chunks
            
        except Exception as e:
            logger.error(f"âŒ Erro no fallback: {e}")
            return []

    def test_search(self):
        """Testa o sistema avanÃ§ado com consulta especÃ­fica"""
        query = "dano moral negativaÃ§Ã£o indevida"
        
        print("\n" + "=" * 60)
        print("ğŸ§ª TESTE DE BUSCA AVANÃ‡ADA EM TEMPO REAL - TJSP")
        print("=" * 60)
        print(f"ğŸ” Consulta: '{query}'")
        print("-" * 60)
        
        # Mostrar estatÃ­sticas iniciais
        stats = self.get_stats()
        print(f"ğŸ“Š EstatÃ­sticas iniciais: {json.dumps(stats, indent=2)}")
        
        # Executar busca
        chunks = self.get_relevant_chunks(query)
        
        if chunks:
            print(f"\nâœ… Encontrados {len(chunks)} chunks relevantes:")
            
            for chunk in chunks:
                print(f"\nğŸ“„ #{chunk['rank']} - Score: {chunk['score']:.3f}")
                print(f"ğŸ“ Fonte: {chunk['source']}")
                print(f"âš–ï¸ Processo: {chunk['processo']}")
                print(f"ğŸ‘¨â€âš–ï¸ Relator: {chunk['relator']}")
                print(f"ğŸ“… Data: {chunk['data']}")
                
                if chunk.get('url'):
                    print(f"ğŸ”— URL: {chunk['url']}")
                
                print(f"ğŸ“ Trecho: {chunk['text'][:300]}...")
                
        else:
            print("âŒ Nenhum resultado encontrado")
        
        # Mostrar estatÃ­sticas finais
        final_stats = self.get_stats()
        print(f"\nğŸ“Š EstatÃ­sticas finais:")
        print(f"   ğŸ¯ Taxa de sucesso: {final_stats['success_rate']:.1f}%")
        print(f"   ğŸš« Bloqueios anti-bot: {final_stats['anti_bot_blocks']}")
        print(f"   ğŸ”„ Uso de fallback: {final_stats['fallback_usage']}")
        print(f"   ğŸ“‹ Itens em cache: {final_stats['cache_size']}")
        
        print("\n" + "=" * 60)


# Wrapper para compatibilidade com cÃ³digo existente
class RealtimeJurisprudenceSearch(AdvancedRealtimeSearch):
    """Wrapper para manter compatibilidade com cÃ³digo existente"""
    pass


def main():
    """FunÃ§Ã£o principal para teste"""
    try:
        # Criar sistema de busca
        search_system = RealtimeJurisprudenceSearch(max_results=10)
        
        # Executar teste
        search_system.test_search()
        
    except Exception as e:
        logger.error(f"âŒ Erro no teste: {e}")


if __name__ == "__main__":
    main()